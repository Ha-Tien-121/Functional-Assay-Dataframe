{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1O5szhgDhh36",
        "outputId": "b5a42d26-1103-459a-d3af-aaa0e3ffb880"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/BBI_CVD_RECLASS/Gene_Maps')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7e4b789",
        "outputId": "62d81d8a-fefa-401b-c6cb-8b1f2f14720d"
      },
      "source": [
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "from Bio.Seq import Seq\n",
        "from Bio.Data import CodonTable\n",
        "import math\n",
        "import os\n",
        "\n",
        "def get_gene_reference_data(gene_symbol, transcript_format='ensembl', specific_transcript=None):\n",
        "    \"\"\"\n",
        "    Get gene transcript ID and reference sequences\n",
        "\n",
        "    Args:\n",
        "        gene_symbol: Gene symbol (e.g., 'MSH2', 'BRCA1', 'TP53')\n",
        "        transcript_format: 'ensembl' or 'refseq'\n",
        "        specific_transcript: Specific transcript ID to use instead of MANE Select\n",
        "    \"\"\"\n",
        "    server = \"https://rest.ensembl.org\"\n",
        "\n",
        "    if specific_transcript:\n",
        "        print(f\"Using specific transcript: {specific_transcript}\")\n",
        "        transcript_id = specific_transcript\n",
        "    else:\n",
        "        # Get gene information to find MANE Select\n",
        "        print(f\"Getting {gene_symbol} gene information...\")\n",
        "        ext = f\"/lookup/symbol/homo_sapiens/{gene_symbol}?expand=1\"\n",
        "\n",
        "        r = requests.get(server + ext,\n",
        "                        headers={\"Content-Type\": \"application/json\"})\n",
        "\n",
        "        if r.status_code != 200:\n",
        "            print(f\"Error {r.status_code}: Could not find gene '{gene_symbol}'\")\n",
        "            print(\"Please check the gene symbol and try again\")\n",
        "            return None\n",
        "\n",
        "        gene_data = r.json()\n",
        "        print(f\"Found {len(gene_data.get('Transcript', []))} transcripts for {gene_symbol}\")\n",
        "\n",
        "        # Find canonical/MANE Select transcript\n",
        "        mane_transcript = None\n",
        "        for transcript in gene_data.get('Transcript', []):\n",
        "            if transcript.get('is_canonical') == 1:\n",
        "                mane_transcript = transcript\n",
        "                break\n",
        "\n",
        "        if not mane_transcript:\n",
        "            mane_transcript = gene_data['Transcript'][0]\n",
        "            print(f\"Using first transcript as backup: {mane_transcript['id']}\")\n",
        "        else:\n",
        "            print(f\"Found canonical transcript: {mane_transcript['id']}\")\n",
        "\n",
        "        ensembl_transcript_id = mane_transcript['id']\n",
        "\n",
        "        # Convert to requested format\n",
        "        if transcript_format.lower() == 'ensembl':\n",
        "            transcript_id = ensembl_transcript_id\n",
        "            print(f\"Using Ensembl format: {transcript_id}\")\n",
        "        elif transcript_format.lower() == 'refseq':\n",
        "            # Get the RefSeq ID\n",
        "            transcript_details = get_transcript_refseq_id(ensembl_transcript_id)\n",
        "            if transcript_details and 'refseq_id' in transcript_details:\n",
        "                transcript_id = transcript_details['refseq_id']\n",
        "                print(f\"Using RefSeq format: {transcript_id}\")\n",
        "            else:\n",
        "                # Try known RefSeq ID\n",
        "                known_refseq = get_known_refseq_id(gene_symbol)\n",
        "                if known_refseq:\n",
        "                    transcript_id = known_refseq\n",
        "                    print(f\"Using known RefSeq ID: {transcript_id}\")\n",
        "                else:\n",
        "                    print(f\"‚ö†Ô∏è Could not find RefSeq ID for {gene_symbol}\")\n",
        "                    print(f\"Using Ensembl ID instead: {ensembl_transcript_id}\")\n",
        "                    transcript_id = ensembl_transcript_id\n",
        "        else:\n",
        "            raise ValueError(\"transcript_format must be 'ensembl' or 'refseq'\")\n",
        "\n",
        "    # Get the actual CDS sequence (use Ensembl ID for API call)\n",
        "    api_transcript_id = specific_transcript if specific_transcript else ensembl_transcript_id\n",
        "    print(f\"Fetching coding sequence for {api_transcript_id}...\")\n",
        "    ext = f\"/sequence/id/{api_transcript_id}?type=cds\"\n",
        "\n",
        "    r = requests.get(server + ext,\n",
        "                    headers={\"Content-Type\": \"text/plain\"})\n",
        "\n",
        "    if r.status_code == 200:\n",
        "        cds_sequence = r.text.strip()\n",
        "\n",
        "        # Translate to get protein sequence\n",
        "        bio_seq = Seq(cds_sequence)\n",
        "        protein_sequence = str(bio_seq.translate())\n",
        "\n",
        "        # Remove stop codon if present\n",
        "        if protein_sequence.endswith('*'):\n",
        "            protein_sequence = protein_sequence[:-1]\n",
        "\n",
        "        return {\n",
        "            'transcript_id': transcript_id,\n",
        "            'gene_symbol': gene_symbol,\n",
        "            'cds_sequence': cds_sequence,\n",
        "            'protein_sequence': protein_sequence,\n",
        "            'cds_length': len(cds_sequence),\n",
        "            'protein_length': len(protein_sequence),\n",
        "            'format': transcript_format\n",
        "        }\n",
        "    else:\n",
        "        print(f\"Error getting sequence: {r.status_code}\")\n",
        "        return None\n",
        "\n",
        "def get_transcript_refseq_id(ensembl_id):\n",
        "    \"\"\"Get RefSeq ID for an Ensembl transcript\"\"\"\n",
        "    server = \"https://rest.ensembl.org\"\n",
        "    ext = f\"/lookup/id/{ensembl_id}?expand=1\"\n",
        "\n",
        "    try:\n",
        "        r = requests.get(server + ext, headers={\"Content-Type\": \"application/json\"})\n",
        "        if r.status_code == 200:\n",
        "            data = r.json()\n",
        "            if 'DBLinks' in data:\n",
        "                for db_link in data['DBLinks']:\n",
        "                    if db_link.get('dbname') == 'RefSeq_mRNA':\n",
        "                        return {'refseq_id': db_link.get('primary_id')}\n",
        "        return None\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def get_known_refseq_id(gene_symbol):\n",
        "    \"\"\"Get known RefSeq MANE Select IDs for common genes\"\"\"\n",
        "    known_refseq_ids = {\n",
        "        'MSH2': 'NM_000251.3', 'BRCA1': 'NM_007294.4', 'BRCA2': 'NM_000059.4',\n",
        "        'TP53': 'NM_000546.6', 'PTEN': 'NM_000314.8', 'VHL': 'NM_000551.4',\n",
        "        'MLH1': 'NM_000249.4', 'MSH6': 'NM_000179.3', 'PMS2': 'NM_000535.7',\n",
        "        'APC': 'NM_000038.6', 'ATM': 'NM_000051.4', 'CHEK2': 'NM_007194.4',\n",
        "        'PALB2': 'NM_024675.4', 'CDH1': 'NM_004360.5'\n",
        "    }\n",
        "    return known_refseq_ids.get(gene_symbol.upper())\n",
        "\n",
        "def generate_nucleotide_variants(transcript_id, cds_sequence):\n",
        "    \"\"\"Generate all possible single nucleotide substitutions\"\"\"\n",
        "    nucleotides = ['A', 'T', 'G', 'C']\n",
        "    variants = []\n",
        "\n",
        "    print(f\"Generating nucleotide variants from {len(cds_sequence)} bp sequence...\")\n",
        "\n",
        "    for pos in range(len(cds_sequence)):\n",
        "        original_nt = cds_sequence[pos]\n",
        "\n",
        "        for new_nt in nucleotides:\n",
        "            if new_nt != original_nt:\n",
        "                hgvs_cdna = f\"{transcript_id}:c.{pos + 1}{original_nt}>{new_nt}\"\n",
        "\n",
        "                variants.append({\n",
        "                    'transcript_id': transcript_id,\n",
        "                    'position': pos + 1,\n",
        "                    'original_nt': original_nt,\n",
        "                    'variant_nt': new_nt,\n",
        "                    'hgvs_cdna': hgvs_cdna,\n",
        "                    'change_type': 'substitution'\n",
        "                })\n",
        "\n",
        "    return variants\n",
        "\n",
        "def generate_all_codon_variants(transcript_id, protein_sequence):\n",
        "    \"\"\"\n",
        "    Generate variants using ALL 64 possible codons (including synonymous and stop)\n",
        "    This creates comprehensive codon-level variants for each position\n",
        "    \"\"\"\n",
        "    # All 64 possible codons\n",
        "    nucleotides = ['A', 'T', 'G', 'C']\n",
        "    all_codons = []\n",
        "    for nt1 in nucleotides:\n",
        "        for nt2 in nucleotides:\n",
        "            for nt3 in nucleotides:\n",
        "                all_codons.append(nt1 + nt2 + nt3)\n",
        "\n",
        "    # Standard genetic code for translation\n",
        "    standard_table = CodonTable.unambiguous_dna_by_id[1]\n",
        "\n",
        "    variants = []\n",
        "\n",
        "    print(f\"Generating ALL codon variants from {len(protein_sequence)} aa sequence...\")\n",
        "    print(f\"Creating variants with all {len(all_codons)} possible codons at each position...\")\n",
        "\n",
        "    for aa_pos in range(len(protein_sequence)):\n",
        "        original_aa = protein_sequence[aa_pos]  # Actual reference amino acid\n",
        "\n",
        "        # Calculate corresponding cDNA positions for this codon\n",
        "        codon_start = aa_pos * 3 + 1  # 1-based position\n",
        "        codon_end = codon_start + 2\n",
        "\n",
        "        # Generate variants using ALL 64 possible codons\n",
        "        for new_codon in all_codons:\n",
        "            # Translate the new codon to see what amino acid it codes for\n",
        "            if new_codon in standard_table.forward_table:\n",
        "                new_aa = standard_table.forward_table[new_codon]\n",
        "                variant_type = 'missense' if new_aa != original_aa else 'synonymous'\n",
        "            elif new_codon in standard_table.stop_codons:\n",
        "                new_aa = '*'  # Stop codon\n",
        "                variant_type = 'nonsense' if original_aa != '*' else 'synonymous_stop'\n",
        "            else:\n",
        "                continue  # Skip invalid codons (shouldn't happen with standard genetic code)\n",
        "\n",
        "            # HGVS protein nomenclature\n",
        "            if new_aa == '*':\n",
        "                hgvs_protein = f\"{transcript_id}:p.{original_aa}{aa_pos + 1}Ter\"\n",
        "            else:\n",
        "                hgvs_protein = f\"{transcript_id}:p.{original_aa}{aa_pos + 1}{new_aa}\"\n",
        "\n",
        "            # HGVS cDNA using delins format\n",
        "            hgvs_cdna = f\"{transcript_id}:c.{codon_start}_{codon_end}delins{new_codon}\"\n",
        "\n",
        "            variants.append({\n",
        "                'transcript_id': transcript_id,\n",
        "                'aa_position': aa_pos + 1,  # 1-based position\n",
        "                'codon_start_pos': codon_start,\n",
        "                'codon_end_pos': codon_end,\n",
        "                'original_aa': original_aa,\n",
        "                'variant_aa': new_aa,\n",
        "                'variant_codon': new_codon,\n",
        "                'variant_type': variant_type,  # synonymous, missense, nonsense\n",
        "                'hgvs_protein': hgvs_protein,\n",
        "                'hgvs_cdna': hgvs_cdna,\n",
        "                'change_type': variant_type\n",
        "            })\n",
        "\n",
        "    return variants\n",
        "\n",
        "def create_vep_input_variants(nt_df=None, aa_df=None, transcript_id=None, variant_types='both'):\n",
        "    \"\"\"Create VEP input variants from DataFrames\"\"\"\n",
        "    vep_variants = []\n",
        "\n",
        "    if variant_types in ['nucleotide', 'both'] and nt_df is not None:\n",
        "        print(f\"Adding {len(nt_df):,} nucleotide variants...\")\n",
        "        for _, row in nt_df.iterrows():\n",
        "            vep_variant = f\"{transcript_id}:c.{row['position']}{row['original_nt']}>{row['variant_nt']}\"\n",
        "            vep_variants.append(vep_variant)\n",
        "\n",
        "    if variant_types in ['codon', 'both'] and aa_df is not None:\n",
        "        print(f\"Adding {len(aa_df):,} codon variants...\")\n",
        "        for _, row in aa_df.iterrows():\n",
        "            vep_variant = f\"{transcript_id}:c.{row['codon_start_pos']}_{row['codon_end_pos']}delins{row['variant_codon']}\"\n",
        "            vep_variants.append(vep_variant)\n",
        "\n",
        "    return vep_variants\n",
        "\n",
        "def split_vep_input_file(input_file_path, chunk_size=10000, output_prefix=\"VEP_chunk\"):\n",
        "    \"\"\"Split a large VEP input file into smaller chunks\"\"\"\n",
        "    print(f\"\\nSplitting VEP input file: {input_file_path}\")\n",
        "    print(f\"Target chunk size: {chunk_size:,} variants\")\n",
        "\n",
        "    with open(input_file_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    variant_lines = [line.strip() for line in lines if line.strip() and not line.startswith('#')]\n",
        "    total_variants = len(variant_lines)\n",
        "    num_chunks = math.ceil(total_variants / chunk_size)\n",
        "\n",
        "    print(f\"Total variants: {total_variants:,}\")\n",
        "    print(f\"Creating {num_chunks} chunk files...\")\n",
        "\n",
        "    chunk_files = []\n",
        "\n",
        "    for chunk_num in range(num_chunks):\n",
        "        start_idx = chunk_num * chunk_size\n",
        "        end_idx = min((chunk_num + 1) * chunk_size, total_variants)\n",
        "\n",
        "        chunk_variants = variant_lines[start_idx:end_idx]\n",
        "        chunk_filename = f\"{output_prefix}_{chunk_num + 1:02d}_of_{num_chunks:02d}.txt\"\n",
        "\n",
        "        with open(chunk_filename, 'w') as f:\n",
        "            for variant in chunk_variants:\n",
        "                f.write(f\"{variant}\\n\")\n",
        "\n",
        "        chunk_files.append(chunk_filename)\n",
        "        print(f\"  Created {chunk_filename}: {len(chunk_variants):,} variants\")\n",
        "\n",
        "    print(f\"\\n‚úÖ Successfully created {len(chunk_files)} chunk files\")\n",
        "\n",
        "    # Create summary file\n",
        "    summary_filename = f\"{output_prefix}_summary.txt\"\n",
        "    with open(summary_filename, 'w') as f:\n",
        "        f.write(f\"VEP Input File Splitting Summary\\n\")\n",
        "        f.write(f\"================================\\n\\n\")\n",
        "        f.write(f\"Original file: {input_file_path}\\n\")\n",
        "        f.write(f\"Total variants: {total_variants:,}\\n\")\n",
        "        f.write(f\"Chunk size: {chunk_size:,}\\n\")\n",
        "        f.write(f\"Number of chunks: {num_chunks}\\n\\n\")\n",
        "        f.write(f\"Chunk Files:\\n\")\n",
        "        for i, filename in enumerate(chunk_files):\n",
        "            start_idx = i * chunk_size\n",
        "            end_idx = min((i + 1) * chunk_size, total_variants)\n",
        "            f.write(f\"  {filename}: variants {start_idx + 1:,} - {end_idx:,}\\n\")\n",
        "\n",
        "    print(f\"Created summary: {summary_filename}\")\n",
        "    return chunk_files, summary_filename\n",
        "\n",
        "def merge_vep_chunk_results(chunk_result_files, output_filename=\"merged_VEP_results.txt\"):\n",
        "    \"\"\"Merge multiple VEP result files back into one\"\"\"\n",
        "    print(f\"Merging {len(chunk_result_files)} VEP result files...\")\n",
        "\n",
        "    all_lines = []\n",
        "    header_written = False\n",
        "\n",
        "    for i, result_file in enumerate(chunk_result_files):\n",
        "        print(f\"  Processing {result_file}...\")\n",
        "\n",
        "        with open(result_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        header_line = None\n",
        "        data_lines = []\n",
        "\n",
        "        for line in lines:\n",
        "            if line.startswith('#Uploaded_variation'):\n",
        "                header_line = line\n",
        "            elif not line.startswith('#') and line.strip():\n",
        "                data_lines.append(line)\n",
        "\n",
        "        if not header_written and header_line:\n",
        "            all_lines.append(header_line)\n",
        "            header_written = True\n",
        "\n",
        "        all_lines.extend(data_lines)\n",
        "        print(f\"    Added {len(data_lines):,} variants\")\n",
        "\n",
        "    with open(output_filename, 'w') as f:\n",
        "        f.writelines(all_lines)\n",
        "\n",
        "    total_variants = len([line for line in all_lines if not line.startswith('#')])\n",
        "    print(f\"\\n‚úÖ Created merged file: {output_filename}\")\n",
        "    print(f\"üìä Total variants: {total_variants:,}\")\n",
        "\n",
        "    return output_filename\n",
        "\n",
        "def complete_variant_pipeline(gene_symbol,\n",
        "                            transcript_format='refseq',\n",
        "                            variant_types='codon',\n",
        "                            chunk_size=10000,\n",
        "                            specific_transcript=None,\n",
        "                            export_dataframes=True):\n",
        "    \"\"\"\n",
        "    Complete pipeline: Generate variant libraries and create VEP input files\n",
        "\n",
        "    Args:\n",
        "        gene_symbol: Gene symbol (e.g., 'MSH2', 'BRCA1', 'TP53')\n",
        "        transcript_format: 'ensembl' or 'refseq'\n",
        "        variant_types: 'nucleotide', 'codon', or 'both'\n",
        "        chunk_size: Number of variants per chunk (None to disable chunking)\n",
        "        specific_transcript: Specific transcript ID instead of MANE Select\n",
        "        export_dataframes: Whether to export variant DataFrames to CSV\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with results and file paths\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"üöÄ COMPLETE {gene_symbol} VARIANT PIPELINE\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"üìç Gene: {gene_symbol.upper()}\")\n",
        "    print(f\"üìç Transcript format: {transcript_format.upper()}\")\n",
        "    print(f\"üìç Variant types: {variant_types.upper()}\")\n",
        "    if variant_types == 'codon':\n",
        "        print(f\"üìç Codon coverage: ALL 64 codons (synonymous + missense + nonsense)\")\n",
        "    if specific_transcript:\n",
        "        print(f\"üìç Specific transcript: {specific_transcript}\")\n",
        "    else:\n",
        "        print(f\"üìç Transcript: MANE Select (auto-detected)\")\n",
        "    if chunk_size:\n",
        "        print(f\"üìç Chunk size: {chunk_size:,} variants\")\n",
        "    else:\n",
        "        print(f\"üìç Chunking: DISABLED\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Initialize chunk_files to an empty list to prevent UnboundLocalError\n",
        "    chunk_files = [] # Initialize local chunk_files variable\n",
        "    summary_file = None\n",
        "\n",
        "    # Step 1: Get reference data\n",
        "    print(\"\\nüîç STEP 1: Getting reference data...\")\n",
        "    ref_data = get_gene_reference_data(gene_symbol, transcript_format, specific_transcript)\n",
        "    if not ref_data:\n",
        "        return None\n",
        "\n",
        "    transcript_id = ref_data['transcript_id']\n",
        "    print(f\"\\n‚úÖ Reference data retrieved:\")\n",
        "    print(f\"   - Transcript: {transcript_id}\")\n",
        "    print(f\"   - CDS length: {ref_data['cds_length']} bp\")\n",
        "    print(f\"   - Protein length: {ref_data['protein_length']} aa\")\n",
        "\n",
        "    # Step 2: Generate variant DataFrames\n",
        "    print(f\"\\nüß¨ STEP 2: Generating variant libraries...\")\n",
        "\n",
        "    nt_df = None\n",
        "    aa_df = None\n",
        "\n",
        "    if variant_types in ['nucleotide', 'both']:\n",
        "        nt_variants = generate_nucleotide_variants(transcript_id, ref_data['cds_sequence'])\n",
        "        nt_df = pd.DataFrame(nt_variants)\n",
        "        print(f\"‚úÖ Generated {len(nt_variants):,} nucleotide variants\")\n",
        "\n",
        "    if variant_types in ['codon', 'both']:\n",
        "        aa_variants = generate_all_codon_variants(transcript_id, ref_data['protein_sequence'])\n",
        "        aa_df = pd.DataFrame(aa_variants)\n",
        "        print(f\"‚úÖ Generated {len(aa_variants):,} codon variants\")\n",
        "\n",
        "        # Show variant type breakdown\n",
        "        if len(aa_variants) > 0:\n",
        "            variant_breakdown = aa_df['variant_type'].value_counts()\n",
        "            print(f\"   Breakdown:\")\n",
        "            for vtype, count in variant_breakdown.items():\n",
        "                print(f\"   - {vtype}: {count:,} variants\")\n",
        "\n",
        "    # Step 3: Export DataFrames (optional)\n",
        "    if export_dataframes:\n",
        "        print(f\"\\nüìÅ STEP 3: Exporting variant DataFrames...\")\n",
        "        clean_transcript = transcript_id.replace('.', '_')\n",
        "\n",
        "        if nt_df is not None:\n",
        "            nt_filename = f\"{gene_symbol}_nucleotide_variants_{clean_transcript}.csv\"\n",
        "            nt_df.to_csv(nt_filename, index=False)\n",
        "            print(f\"   Exported: {nt_filename}\")\n",
        "\n",
        "        if aa_df is not None:\n",
        "            aa_filename = f\"{gene_symbol}_codon_variants_all64_{clean_transcript}.csv\"\n",
        "            aa_df.to_csv(aa_filename, index=False)\n",
        "            print(f\"   Exported: {aa_filename}\")\n",
        "\n",
        "    # Step 4: Create VEP input file\n",
        "    print(f\"\\nüéØ STEP 4: Creating VEP input file...\")\n",
        "    vep_variants = create_vep_input_variants(nt_df, aa_df, transcript_id, variant_types)\n",
        "\n",
        "    if len(vep_variants) == 0:\n",
        "        print(\"‚ùå No variants created\")\n",
        "        return None\n",
        "\n",
        "    # Generate filenames\n",
        "    clean_transcript = transcript_id.replace('.', '_')\n",
        "    variant_suffix = variant_types.lower().replace('_', '')\n",
        "    format_suffix = transcript_format.lower()\n",
        "\n",
        "    if variant_types == 'codon':\n",
        "        variant_suffix = 'all64codons'\n",
        "\n",
        "    if specific_transcript:\n",
        "        output_prefix = f\"{gene_symbol}_{variant_suffix}_VEP_{format_suffix}_{clean_transcript}_custom\"\n",
        "    else:\n",
        "        output_prefix = f\"{gene_symbol}_{variant_suffix}_VEP_{format_suffix}_{clean_transcript}\"\n",
        "\n",
        "    # Create full VEP input file\n",
        "    full_filename = f\"{output_prefix}_full.txt\"\n",
        "    with open(full_filename, 'w') as f:\n",
        "        for variant in vep_variants:\n",
        "            f.write(f\"{variant}\\n\")\n",
        "\n",
        "    print(f\"‚úÖ Created VEP input file: {full_filename} ({len(vep_variants):,} variants)\")\n",
        "\n",
        "    results = {\n",
        "        'gene_symbol': gene_symbol,\n",
        "        'transcript_id': transcript_id,\n",
        "        'transcript_format': transcript_format,\n",
        "        'variant_types': variant_types,\n",
        "        'specific_transcript': specific_transcript,\n",
        "        'total_variants': len(vep_variants),\n",
        "        'nt_df': nt_df,\n",
        "        'aa_df': aa_df,\n",
        "        'ref_data': ref_data,\n",
        "        'full_vep_file': full_filename,\n",
        "        'chunk_files': [],\n",
        "        'summary_file': None\n",
        "    }\n",
        "\n",
        "    # Step 5: Split into chunks if requested\n",
        "    if chunk_size and chunk_size > 0:\n",
        "        print(f\"\\n‚úÇÔ∏è STEP 5: Splitting into {chunk_size:,} variant chunks...\")\n",
        "        chunk_prefix = f\"{output_prefix}_chunk\"\n",
        "        chunk_files, summary_file = split_vep_input_file(full_filename, chunk_size, chunk_prefix)\n",
        "        results['chunk_files'] = chunk_files\n",
        "        results['summary_file'] = summary_file\n",
        "\n",
        "    # Step 6: Show processing instructions\n",
        "    print(f\"\\nüìù VEP PROCESSING INSTRUCTIONS:\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    if chunk_files:\n",
        "        print(f\"1. Upload each chunk file to VEP web interface separately:\")\n",
        "        for chunk_file in chunk_files[:3]:\n",
        "            print(f\"   - {chunk_file}\")\n",
        "        if len(chunk_files) > 3:\n",
        "            print(f\"   - ... and {len(chunk_files) - 3} more\")\n",
        "    else:\n",
        "        print(f\"1. Upload {full_filename} to VEP web interface\")\n",
        "\n",
        "    print(f\"2. VEP Settings:\")\n",
        "    print(f\"   - Transcript database: {transcript_format.title()}\")\n",
        "    if transcript_format.lower() == 'refseq':\n",
        "        print(f\"   - Select 'RefSeq transcripts' in VEP\")\n",
        "    else:\n",
        "        print(f\"   - Select 'Ensembl transcripts' in VEP (default)\")\n",
        "\n",
        "    if chunk_files:\n",
        "        print(f\"3. Use identical settings for all {len(chunk_files)} chunks\")\n",
        "        print(f\"4. Download results as: chunk_01_results.txt, chunk_02_results.txt, etc.\")\n",
        "        print(f\"5. Merge results: merge_vep_chunk_results(['chunk_01_results.txt', ...])\")\n",
        "    else:\n",
        "        print(f\"3. Download results when complete\")\n",
        "\n",
        "    # Calculate expected counts\n",
        "    expected_nt = ref_data['cds_length'] * 3 if variant_types in ['nucleotide', 'both'] else 0\n",
        "    expected_codon = ref_data['protein_length'] * 64 if variant_types in ['codon', 'both'] else 0\n",
        "\n",
        "    print(f\"\\n‚úÖ PIPELINE COMPLETE!\")\n",
        "    print(f\"üìä Summary:\")\n",
        "    print(f\"   - Gene: {gene_symbol}\")\n",
        "    print(f\"   - Transcript: {transcript_id}\")\n",
        "    if variant_types in ['nucleotide', 'both']:\n",
        "        print(f\"   - Nucleotide variants: {len(nt_df) if nt_df is not None else 0:,} (expected: {expected_nt:,})\")\n",
        "    if variant_types in ['codon', 'both']:\n",
        "        print(f\"   - Codon variants: {len(aa_df) if aa_df is not None else 0:,} (expected: {expected_codon:,})\")\n",
        "    print(f\"   - Total VEP variants: {len(vep_variants):,}\")\n",
        "    print(f\"   - VEP input file: {full_filename}\")\n",
        "    if chunk_files:\n",
        "        print(f\"   - Chunk files: {len(chunk_files)}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Usage examples and help\n",
        "print(\"üöÄ COMPLETE VARIANT PIPELINE WITH ALL 64 CODONS READY!\")\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"USAGE EXAMPLES:\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "print(\"\\n1. MSH2 ALL CODONS (RefSeq, 10K chunks) - COMPREHENSIVE:\")\n",
        "print(\"   results = complete_variant_pipeline('MSH2')\")\n",
        "print(\"   # Creates synonymous + missense + nonsense variants\")\n",
        "\n",
        "print(\"\\n2. BRCA1 NUCLEOTIDE + ALL CODONS (Ensembl, 5K chunks):\")\n",
        "print(\"   results = complete_variant_pipeline(\")\n",
        "print(\"       gene_symbol='BRCA1',\")\n",
        "print(\"       transcript_format='ensembl',\")\n",
        "print(\"       variant_types='both',\")\n",
        "print(\"       chunk_size=5000\")\n",
        "print(\"   )\")\n",
        "\n",
        "print(\"\\n3. TP53 ALL CODONS ONLY (RefSeq, no chunking):\")\n",
        "print(\"   results = complete_variant_pipeline(\")\n",
        "print(\"       gene_symbol='TP53',\")\n",
        "print(\"       variant_types='codon',\")\n",
        "print(\"       chunk_size=None\")\n",
        "print(\"   )\")\n",
        "\n",
        "print(\"\\n4. CUSTOM TRANSCRIPT:\")\n",
        "print(\"   results = complete_variant_pipeline(\")\n",
        "print(\"       gene_symbol='PTEN',\")\n",
        "print(\"       transcript_format='ensembl',\")\n",
        "print(\"       specific_transcript='ENST00000371953.8'\")\n",
        "print(\"   )\")\n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ COMPLETE VARIANT PIPELINE WITH ALL 64 CODONS READY!\n",
            "\n",
            "====================================================================================================\n",
            "USAGE EXAMPLES:\n",
            "====================================================================================================\n",
            "\n",
            "1. MSH2 ALL CODONS (RefSeq, 10K chunks) - COMPREHENSIVE:\n",
            "   results = complete_variant_pipeline('MSH2')\n",
            "   # Creates synonymous + missense + nonsense variants\n",
            "\n",
            "2. BRCA1 NUCLEOTIDE + ALL CODONS (Ensembl, 5K chunks):\n",
            "   results = complete_variant_pipeline(\n",
            "       gene_symbol='BRCA1',\n",
            "       transcript_format='ensembl',\n",
            "       variant_types='both',\n",
            "       chunk_size=5000\n",
            "   )\n",
            "\n",
            "3. TP53 ALL CODONS ONLY (RefSeq, no chunking):\n",
            "   results = complete_variant_pipeline(\n",
            "       gene_symbol='TP53',\n",
            "       variant_types='codon',\n",
            "       chunk_size=None\n",
            "   )\n",
            "\n",
            "4. CUSTOM TRANSCRIPT:\n",
            "   results = complete_variant_pipeline(\n",
            "       gene_symbol='PTEN',\n",
            "       transcript_format='ensembl',\n",
            "       specific_transcript='ENST00000371953.8'\n",
            "   )\n",
            "\n",
            "====================================================================================================\n",
            "KEY FEATURES:\n",
            "====================================================================================================\n",
            "üß¨ ALL 64 CODONS: Includes synonymous, missense, and nonsense variants\n",
            "üìä VARIANT BREAKDOWN: Shows counts for each variant type\n",
            "üéØ COMPREHENSIVE COVERAGE: Every possible codon at every position\n",
            "‚ö° HUGE LIBRARIES: ~60K variants per gene (protein_length √ó 64)\n",
            "üîß FLEXIBLE: Choose nucleotide, codon, or both\n",
            "\n",
            "üí° RECOMMENDED FOR FUNCTIONAL STUDIES:\n",
            "   results = complete_variant_pipeline('MSH2', variant_types='codon')\n",
            "\n",
            "   This gives you the most comprehensive functional variant library!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab609198",
        "outputId": "0560ee5f-2ebc-4b39-f8dd-b8ef4aff5167"
      },
      "source": [
        "results = complete_variant_pipeline(gene_symbol='MSH2', chunk_size=25000, variant_types='codon')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ COMPLETE MSH2 VARIANT PIPELINE - ALL 64 CODONS!\n",
            "================================================================================\n",
            "üìç Gene: MSH2\n",
            "üìç Transcript format: REFSEQ\n",
            "üìç Variant types: CODON\n",
            "üìç Codon coverage: ALL 64 codons (synonymous + missense + nonsense)\n",
            "üìç Transcript: MANE Select (auto-detected)\n",
            "üìç Chunk size: 25,000 variants\n",
            "================================================================================\n",
            "\n",
            "üîç STEP 1: Getting reference data...\n",
            "Getting MSH2 gene information...\n",
            "Found 26 transcripts for MSH2\n",
            "Found canonical transcript: ENST00000233146\n",
            "Using known RefSeq ID: NM_000251.3\n",
            "Fetching coding sequence for ENST00000233146...\n",
            "\n",
            "‚úÖ Reference data retrieved:\n",
            "   - Transcript: NM_000251.3\n",
            "   - CDS length: 2805 bp\n",
            "   - Protein length: 934 aa\n",
            "\n",
            "üß¨ STEP 2: Generating variant libraries...\n",
            "Generating ALL codon variants from 934 aa sequence...\n",
            "Creating variants with all 64 possible codons at each position...\n",
            "‚úÖ Generated 59,776 codon variants\n",
            "   Breakdown:\n",
            "   - missense: 53,800 variants\n",
            "   - synonymous: 3,174 variants\n",
            "   - nonsense: 2,802 variants\n",
            "\n",
            "üìÅ STEP 3: Exporting variant DataFrames...\n",
            "   Exported: MSH2_codon_variants_all64_NM_000251_3.csv\n",
            "\n",
            "üéØ STEP 4: Creating VEP input file...\n",
            "Adding 59,776 codon variants...\n",
            "‚úÖ Created VEP input file: MSH2_all64codons_VEP_refseq_NM_000251_3_full.txt (59,776 variants)\n",
            "\n",
            "‚úÇÔ∏è STEP 5: Splitting into 25,000 variant chunks...\n",
            "\n",
            "Splitting VEP input file: MSH2_all64codons_VEP_refseq_NM_000251_3_full.txt\n",
            "Target chunk size: 25,000 variants\n",
            "Total variants: 59,776\n",
            "Creating 3 chunk files...\n",
            "  Created MSH2_all64codons_VEP_refseq_NM_000251_3_chunk_01_of_03.txt: 25,000 variants\n",
            "  Created MSH2_all64codons_VEP_refseq_NM_000251_3_chunk_02_of_03.txt: 25,000 variants\n",
            "  Created MSH2_all64codons_VEP_refseq_NM_000251_3_chunk_03_of_03.txt: 9,776 variants\n",
            "\n",
            "‚úÖ Successfully created 3 chunk files\n",
            "Created summary: MSH2_all64codons_VEP_refseq_NM_000251_3_chunk_summary.txt\n",
            "\n",
            "üìù VEP PROCESSING INSTRUCTIONS:\n",
            "==================================================\n",
            "1. Upload each chunk file to VEP web interface separately:\n",
            "   - MSH2_all64codons_VEP_refseq_NM_000251_3_chunk_01_of_03.txt\n",
            "   - MSH2_all64codons_VEP_refseq_NM_000251_3_chunk_02_of_03.txt\n",
            "   - MSH2_all64codons_VEP_refseq_NM_000251_3_chunk_03_of_03.txt\n",
            "2. VEP Settings:\n",
            "   - Transcript database: Refseq\n",
            "   - Select 'RefSeq transcripts' in VEP\n",
            "3. Use identical settings for all 3 chunks\n",
            "4. Download results as: chunk_01_results.txt, chunk_02_results.txt, etc.\n",
            "5. Merge results: merge_vep_chunk_results(['chunk_01_results.txt', ...])\n",
            "\n",
            "‚úÖ PIPELINE COMPLETE!\n",
            "üìä Summary:\n",
            "   - Gene: MSH2\n",
            "   - Transcript: NM_000251.3\n",
            "   - Codon variants: 59,776 (expected: 59,776)\n",
            "   - Total VEP variants: 59,776\n",
            "   - VEP input file: MSH2_all64codons_VEP_refseq_NM_000251_3_full.txt\n",
            "   - Chunk files: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "def variantvalidator_to_vcf(input_file, output_file, genome_build='hg38'):\n",
        "    \"\"\"\n",
        "    Convert VariantValidator output to VCF format\n",
        "\n",
        "    Parameters:\n",
        "    input_file (str): Path to VariantValidator tab-delimited output file\n",
        "    output_file (str): Path for output VCF file\n",
        "    genome_build (str): Genome build to use ('hg38' or 'hg37')\n",
        "    \"\"\"\n",
        "\n",
        "    # Read the VariantValidator output, skipping the first two rows\n",
        "    df = pd.read_csv(input_file, sep='\\t', skiprows=2)\n",
        "\n",
        "    # Select coordinate columns based on genome build\n",
        "    if genome_build.lower() == 'hg38':\n",
        "        chr_col = 'GRCh38_CHR'\n",
        "        pos_col = 'GRCh38_POS'\n",
        "        ref_col = 'GRCh38_REF'\n",
        "        alt_col = 'GRCh38_ALT'\n",
        "        reference = 'GRCh38'\n",
        "    elif genome_build.lower() == 'hg37':\n",
        "        chr_col = 'GRCh37_CHR'\n",
        "        pos_col = 'GRCh37_POS'\n",
        "        ref_col = 'GRCh37_REF'\n",
        "        alt_col = 'GRCh37_ALT'\n",
        "        reference = 'GRCh37'\n",
        "    else:\n",
        "        raise ValueError(\"genome_build must be 'hg38' or 'hg37'\")\n",
        "\n",
        "    # Filter out rows with missing genomic coordinates\n",
        "    df = df.dropna(subset=[chr_col, pos_col, ref_col, alt_col])\n",
        "\n",
        "    # Create VCF header\n",
        "    vcf_header = f\"\"\"##fileformat=VCFv4.2\n",
        "##reference={reference}\n",
        "##fileDate={datetime.now().strftime('%Y%m%d')}\n",
        "##source=VariantValidator_to_VCF_converter\n",
        "##INFO=<ID=GENE,Number=1,Type=String,Description=\\\"Gene symbol\\\">\n",
        "##INFO=<ID=HGVS_C,Number=1,Type=String,Description=\\\"HGVS cDNA notation\\\">\n",
        "##INFO=<ID=HGVS_P,Number=1,Type=String,Description=\\\"HGVS protein notation\\\">\n",
        "##INFO=<ID=TRANSCRIPT,Number=1,Type=String,Description=\\\"Reference transcript\\\">\n",
        "##INFO=<ID=HGNC_ID,Number=1,Type=String,Description=\\\"HGNC Gene ID\\\">\n",
        "##INFO=<ID=TRANSCRIPT_DESC,Number=1,Type=String,Description=\\\"Transcript description\\\">\"\"\"\n",
        "\n",
        "    # Add contig information for chromosomes present in data\n",
        "    chromosomes = df[chr_col].dropna().unique()\n",
        "    for chrom in sorted(chromosomes):\n",
        "        if chrom == 'X':\n",
        "            length = '156040895' if genome_build.lower() == 'hg38' else '155270560'\n",
        "        elif chrom == 'Y':\n",
        "            length = '57227415' if genome_build.lower() == 'hg38' else '59373566'\n",
        "        elif chrom == 'MT' or chrom == 'M':\n",
        "            length = '16569'\n",
        "        else:\n",
        "            # Approximate lengths for autosomes (you may want to use exact values)\n",
        "            chr_lengths_hg38 = {\n",
        "                '1': '248956422', '2': '242193529', '3': '198295559', '4': '190214555',\n",
        "                '5': '181538259', '6': '170805979', '7': '159345973', '8': '145138636',\n",
        "                '9': '138394717', '10': '133797422', '11': '135086622', '12': '133275309',\n",
        "                '13': '114364328', '14': '107043718', '15': '101991189', '16': '90338345',\n",
        "                '17': '83257441', '18': '80373285', '19': '58617616', '20': '64444167',\n",
        "                '21': '46709983', '22': '50818468'\n",
        "            }\n",
        "            chr_lengths_hg37 = {\n",
        "                '1': '249250621', '2': '242193529', '3': '198022430', '4': '191154276',\n",
        "                '5': '180915260', '6': '171115067', '7': '159138663', '8': '146364022',\n",
        "                '9': '141213431', '10': '135534747', '11': '135006516', '12': '133851895',\n",
        "                '13': '115169878', '14': '107349540', '15': '102531392', '16': '90354753',\n",
        "                '17': '81195210', '18': '78077248', '19': '59128983', '20': '63025520',\n",
        "                '21': '48129895', '22': '51304566'\n",
        "            }\n",
        "\n",
        "            if genome_build.lower() == 'hg38':\n",
        "                length = chr_lengths_hg38.get(str(chrom), '100000000')\n",
        "            else:\n",
        "                length = chr_lengths_hg37.get(str(chrom), '100000000')\n",
        "\n",
        "        vcf_header += f\"\\n##contig=<ID={chrom},length={length}>\"\n",
        "\n",
        "    # Add column header\n",
        "    vcf_header += \"\\n#CHROM\\tPOS\\tID\\tREF\\tALT\\tQUAL\\tFILTER\\tINFO\"\n",
        "\n",
        "    # Process each variant\n",
        "    vcf_records = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        # Extract basic variant information\n",
        "        chrom = str(row[chr_col])\n",
        "        pos = str(int(row[pos_col]))\n",
        "        ref = row[ref_col]\n",
        "        alt = row[alt_col]\n",
        "\n",
        "        # Create INFO field\n",
        "        info_parts = []\n",
        "\n",
        "        if pd.notna(row.get('Gene_Symbol')):\n",
        "            info_parts.append(f\"GENE={row['Gene_Symbol']}\")\n",
        "\n",
        "        if pd.notna(row.get('HGVS_transcript')):\n",
        "            # Clean up HGVS notation by removing any warnings/notes\n",
        "            hgvs_c = row['HGVS_transcript']\n",
        "            info_parts.append(f\"HGVS_C={hgvs_c}\")\n",
        "\n",
        "            # Extract transcript ID from HGVS notation\n",
        "            transcript_match = re.match(r'([^:]+):', hgvs_c)\n",
        "            if transcript_match:\n",
        "                info_parts.append(f\"TRANSCRIPT={transcript_match.group(1)}\")\n",
        "\n",
        "        if pd.notna(row.get('HGVS_Predicted_Protein')):\n",
        "            info_parts.append(f\"HGVS_P={row['HGVS_Predicted_Protein']}\")\n",
        "\n",
        "        if pd.notna(row.get('HGNC_Gene_ID')):\n",
        "            info_parts.append(f\"HGNC_ID={row['HGNC_Gene_ID']}\")\n",
        "\n",
        "        if pd.notna(row.get('Transcript_description')):\n",
        "            # Clean up description and remove spaces/special characters\n",
        "            desc = str(row['Transcript_description']).replace(' ', '_').replace(',', ';')\n",
        "            info_parts.append(f\"TRANSCRIPT_DESC={desc}\")\n",
        "\n",
        "        info_field = ';'.join(info_parts) if info_parts else '.'\n",
        "\n",
        "        # Create VCF record\n",
        "        vcf_record = f\"{chrom}\\t{pos}\\t.\\t{ref}\\t{alt}\\t.\\tPASS\\t{info_field}\"\n",
        "        vcf_records.append(vcf_record)\n",
        "\n",
        "    # Write VCF file\n",
        "    with open(output_file, 'w') as f:\n",
        "        f.write(vcf_header + '\\n')\n",
        "        for record in vcf_records:\n",
        "            f.write(record + '\\n')\n",
        "\n",
        "    print(f\"VCF file created: {output_file}\")\n",
        "    print(f\"Number of variants: {len(vcf_records)}\")\n",
        "    print(f\"Genome build: {reference}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "9nKht01Hy4Ye"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"### Step 2:VEP input file into variant validator batch annotator https://variantvalidator.org/service/validate/batch/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55M7TS7qzZfL",
        "outputId": "ba586cdf-871c-4daf-bee1-a2403008704c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Step 2: paste VCF file into variant validator batch annotator https://variantvalidator.org/service/validate/batch/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba49cc59",
        "outputId": "866a3e41-00a2-4418-8f4a-6e321b283f23"
      },
      "source": [
        "print(\"### Step 3: Convert VariantValidator output to VCF\")\n",
        "\n",
        "# Call the function to convert VariantValidator output to VCF\n",
        "variantvalidator_to_vcf('vv_MSH2_1_of_3.txt', 'MSH2_1_of_3.vcf', genome_build='hg38')\n",
        "variantvalidator_to_vcf('vv_MSH2_2_of_3.txt', 'MSH2_2_of_3.vcf', genome_build='hg38')\n",
        "variantvalidator_to_vcf('vv_MSH2_3_of_3.txt', 'MSH2_3_of_3.vcf', genome_build='hg38')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Step 3: Convert VariantValidator output to VCF\n",
            "VCF file created: MSH2_1_of_3.vcf\n",
            "Number of variants: 25000\n",
            "Genome build: GRCh38\n",
            "VCF file created: MSH2_2_of_3.vcf\n",
            "Number of variants: 25000\n",
            "Genome build: GRCh38\n",
            "VCF file created: MSH2_3_of_3.vcf\n",
            "Number of variants: 9776\n",
            "Genome build: GRCh38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"### Step 4: upload VCF file to Open CRAVAT\" https://run.opencravat.org/submit/nocache/index.html with REVEL, EVE, AlphaMissense, BayesDel, MutPred, ClinVar, Gnomad4, Gnomad3, Gnomad)"
      ],
      "metadata": {
        "id": "7BSOvhBVzxek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cccf2a91",
        "outputId": "3ac72c31-c6c8-4804-babf-bd468b947f27"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def parse_cravat_tsv(file_path):\n",
        "    \"\"\"\n",
        "    Parses a CRAVAT TSV file with a two-level header, combines headers,\n",
        "    cleans column names, and loads the data into a pandas DataFrame.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the CRAVAT TSV file.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame with cleaned and combined column names.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Step 1: Read the header lines to process them ---\n",
        "    # Assuming 192 is the maximum number of columns, based on previous analysis.\n",
        "    # Create a list of placeholder names to force read_csv to parse all columns.\n",
        "    num_expected_cols = 192 # This value might need adjustment if the actual file has more or fewer columns\n",
        "    placeholder_names = [f'col_{i}' for i in range(num_expected_cols)]\n",
        "\n",
        "    # Read the actual first header line (grouping columns), assuming it's the 6th line (index 5) in the file\n",
        "    header_line1_raw = pd.read_csv(file_path, sep='\\t', skiprows=5, nrows=1, header=None, names=placeholder_names).iloc[0]\n",
        "\n",
        "    # Read the actual second header line (specific column names), assuming it's the 7th line (index 6) in the file\n",
        "    header_line2_raw = pd.read_csv(file_path, sep='\\t', skiprows=6, nrows=1, header=None, names=placeholder_names).iloc[0]\n",
        "\n",
        "    # --- Step 2: Combine header lines into a single, flat list of column names ---\n",
        "    combined_columns = []\n",
        "    # Initialize last_major_col to handle cases where initial columns might not have a grouping header\n",
        "    last_major_col = \"\"\n",
        "\n",
        "    # Ensure both headers are lists/series of comparable length; fill NaN values in the first header for consistent processing\n",
        "    # header_line1_filled will now have the same length as header_line2_raw because of 'names' parameter\n",
        "    header_line1_filled = header_line1_raw.ffill().fillna('').astype(str)\n",
        "\n",
        "    for i in range(len(header_line2_raw)): # Iterate through the more detailed second header line\n",
        "        major_col_val = header_line1_filled.iloc[i].strip()\n",
        "        minor_col_val = header_line2_raw.iloc[i]\n",
        "\n",
        "        # Update last_major_col if the current major_col_val is not empty\n",
        "        if major_col_val:\n",
        "            last_major_col = major_col_val\n",
        "\n",
        "        # Combine major and minor column names\n",
        "        if pd.isna(minor_col_val): # If the minor column is NaN, use the last major group\n",
        "            if last_major_col:\n",
        "                combined_columns.append(last_major_col)\n",
        "            elif major_col_val: # Fallback if last_major_col was empty but current major_col has a value\n",
        "                combined_columns.append(major_col_val)\n",
        "            else:\n",
        "                combined_columns.append(f\"Unnamed_Col_{i}\") # Fallback for truly empty/unnamed columns\n",
        "        else:\n",
        "            minor_col_str = str(minor_col_val).strip()\n",
        "            # If a major group exists and the minor column isn't redundant, combine them\n",
        "            if last_major_col and minor_col_str != last_major_col and not minor_col_str.startswith(last_major_col + '_'):\n",
        "                combined_columns.append(f\"{last_major_col}_{minor_col_str}\")\n",
        "            else:\n",
        "                combined_columns.append(minor_col_str)\n",
        "\n",
        "    # Clean up column names: remove leading/trailing spaces, replace problematic characters\n",
        "    cleaned_columns = [col.strip().replace(' ', '_').replace('.', '').replace('/', '_').replace('#', '') for col in combined_columns]\n",
        "\n",
        "    # Handle duplicate column names by appending a suffix (e.g., 'col' and 'col_1')\n",
        "    name_counts = {}\n",
        "    final_columns = []\n",
        "\n",
        "    for col in cleaned_columns:\n",
        "        base_col = col # The original cleaned name\n",
        "        count = name_counts.get(base_col, 0) # Get current count for this base name\n",
        "\n",
        "        # Keep trying new names until a unique one is found\n",
        "        new_col_name = base_col\n",
        "        while new_col_name in final_columns:\n",
        "            count += 1\n",
        "            new_col_name = f\"{base_col}_{count}\"\n",
        "\n",
        "        final_columns.append(new_col_name)\n",
        "        name_counts[base_col] = count # Update the count for the base name\n",
        "\n",
        "\n",
        "    # --- Step 3: Load the data with the new column names ---\n",
        "    # Skip the first 7 rows (5 metadata + 2 header lines) and use the combined column names\n",
        "    cravat_df = pd.read_csv(file_path, sep='\\t', skiprows=7, header=None, names=final_columns, low_memory=False)\n",
        "\n",
        "    return cravat_df\n",
        "\n",
        "# Call the new function with the CRAVAT file\n",
        "cravat_df = parse_cravat_tsv('CRAVAT_MSH2.tsv')\n",
        "\n",
        "# Display the first 5 rows of the DataFrame\n",
        "print(\"First 5 rows of cravat_df using the new function:\")\n",
        "print(cravat_df.head())\n",
        "\n",
        "# Display the column names\n",
        "print(\"\\nColumn names of cravat_df using the new function:\")\n",
        "print(cravat_df.columns)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 rows of cravat_df using the new function:\n",
            "   Variant_Annotation_UID Variant_Annotation_Chrom  \\\n",
            "0                   24803                     chr2   \n",
            "1                   46763                     chr2   \n",
            "2                   19586                     chr2   \n",
            "3                   31243                     chr2   \n",
            "4                   40485                     chr2   \n",
            "\n",
            "   Variant_Annotation_Position Variant_Annotation_Ref_Base  \\\n",
            "0                     47478397                           T   \n",
            "1                     47410289                           G   \n",
            "2                     47476450                           T   \n",
            "3                     47482781                           A   \n",
            "4                     47408458                           A   \n",
            "\n",
            "  Variant_Annotation_Alt_Base  Variant_Annotation_Variant_Note  \\\n",
            "0                           G                              NaN   \n",
            "1                           T                              NaN   \n",
            "2                           G                              NaN   \n",
            "3                           G                              NaN   \n",
            "4                           C                              NaN   \n",
            "\n",
            "  Variant_Annotation_Coding Variant_Annotation_Gene  \\\n",
            "0                       Yes                    MSH2   \n",
            "1                       Yes                    MSH2   \n",
            "2                       Yes                    MSH2   \n",
            "3                       Yes                    MSH2   \n",
            "4                       Yes                    MSH2   \n",
            "\n",
            "  Variant_Annotation_Transcript Variant_Annotation_Sequence_Ontology  ...  \\\n",
            "0             ENST00000233146.7                     missense_variant  ...   \n",
            "1             ENST00000233146.7                          stop_gained  ...   \n",
            "2             ENST00000233146.7                     missense_variant  ...   \n",
            "3             ENST00000233146.7                   synonymous_variant  ...   \n",
            "4             ENST00000233146.7                     missense_variant  ...   \n",
            "\n",
            "   SpliceAI_Donor_Loss_Posiiton VCF_Info_Phred VCF_Info_VCF_filter  \\\n",
            "0                         -13.0            NaN                PASS   \n",
            "1                           3.0            NaN                PASS   \n",
            "2                          44.0            NaN                PASS   \n",
            "3                           1.0            NaN                PASS   \n",
            "4                          -5.0            NaN                PASS   \n",
            "\n",
            "  VCF_Info_Zygosity  VCF_Info_Alternate_reads  VCF_Info_Total_reads  \\\n",
            "0               NaN                       NaN                   NaN   \n",
            "1               NaN                       NaN                   NaN   \n",
            "2               NaN                       NaN                   NaN   \n",
            "3               NaN                       NaN                   NaN   \n",
            "4               NaN                       NaN                   NaN   \n",
            "\n",
            "  VCF_Info_Variant_AF  VCF_Info_Haplotype_block_ID  \\\n",
            "0                 NaN                          NaN   \n",
            "1                 NaN                          NaN   \n",
            "2                 NaN                          NaN   \n",
            "3                 NaN                          NaN   \n",
            "4                 NaN                          NaN   \n",
            "\n",
            "   VCF_Info_Haplotype_strand_ID VCF_Info  \n",
            "0                           NaN      NaN  \n",
            "1                           NaN      NaN  \n",
            "2                           NaN      NaN  \n",
            "3                           NaN      NaN  \n",
            "4                           NaN      NaN  \n",
            "\n",
            "[5 rows x 192 columns]\n",
            "\n",
            "Column names of cravat_df using the new function:\n",
            "Index(['Variant_Annotation_UID', 'Variant_Annotation_Chrom',\n",
            "       'Variant_Annotation_Position', 'Variant_Annotation_Ref_Base',\n",
            "       'Variant_Annotation_Alt_Base', 'Variant_Annotation_Variant_Note',\n",
            "       'Variant_Annotation_Coding', 'Variant_Annotation_Gene',\n",
            "       'Variant_Annotation_Transcript', 'Variant_Annotation_Sequence_Ontology',\n",
            "       ...\n",
            "       'SpliceAI_Donor_Loss_Posiiton', 'VCF_Info_Phred', 'VCF_Info_VCF_filter',\n",
            "       'VCF_Info_Zygosity', 'VCF_Info_Alternate_reads', 'VCF_Info_Total_reads',\n",
            "       'VCF_Info_Variant_AF', 'VCF_Info_Haplotype_block_ID',\n",
            "       'VCF_Info_Haplotype_strand_ID', 'VCF_Info'],\n",
            "      dtype='object', length=192)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "464e1aca",
        "outputId": "16190b48-4b76-4d57-b8c3-dc605e0ba4dc"
      },
      "source": [
        "output_tsv_filename = 'cravat_msh2_cleaned.tsv'\n",
        "cravat_df.to_csv(output_tsv_filename, sep='\\t', index=False)\n",
        "\n",
        "print(f\"DataFrame saved to '{output_tsv_filename}'\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame saved to 'cravat_msh2_cleaned.tsv'\n"
          ]
        }
      ]
    }
  ]
}